{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHtOFCqfuh_5"
      },
      "outputs": [],
      "source": [
        "!pip install torch_snippets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-QHRyd6uWGT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import PIL\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torchvision\n",
        "from torch_snippets import Report\n",
        "import os\n",
        "import time\n",
        "import xml.etree.ElementTree as ET\n",
        "# Connect to the GPU if one exists.\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSH54LqXuicZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"w55UqH87vVgdLEooW6mY\",model_format = 'voc')\n",
        "project = rf.workspace(\"karlsruhe-institute-of-technology-glopj\").project(\"-underwater-images\")\n",
        "dataset = project.version(1).download(\"voc\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWLs_u7bxDP1"
      },
      "source": [
        "# Neuer Abschnitt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9fyae7LuifM"
      },
      "outputs": [],
      "source": [
        "def xml_to_dict(xml_path):\n",
        "    # Decode the .xml file\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "    # Return the image size, object label and bounding box \n",
        "    # coordinates together with the filename as a dict.\n",
        "    return {\"filename\": xml_path,\n",
        "            \"image_width\": int(root.find(\"./size/width\").text),\n",
        "            \"image_height\": int(root.find(\"./size/height\").text),\n",
        "            \"image_channels\": int(root.find(\"./size/depth\").text),\n",
        "            \"label\": root.find(\"./object/name\").text,\n",
        "            \"x1\": int(root.find(\"./object/bndbox/xmin\").text),\n",
        "            \"y1\": int(root.find(\"./object/bndbox/ymin\").text),\n",
        "            \"x2\": int(root.find(\"./object/bndbox/xmax\").text),\n",
        "            \"y2\": int(root.find(\"./object/bndbox/ymax\").text)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rumGCofauihD"
      },
      "outputs": [],
      "source": [
        "# Convert human readable str label to int.\n",
        "label_dict = {\"human\": 1, \"robot\" : 2,\"fish\" :3}\n",
        "# Convert label int to human readable str.\n",
        "reverse_label_dict = {1: \"human\", 2: \"robot\", 3: \"fish\"}\n",
        "class UnderwateImagesDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms = None):\n",
        "        \"\"\"\n",
        "        Inputs\n",
        "            root: str\n",
        "                Path to the data folder.\n",
        "            transforms: Compose or list\n",
        "                Torchvision image transformations.\n",
        "        \"\"\"\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        self.files = sorted(os.listdir(root))\n",
        "        for i in range(len(self.files)):\n",
        "            self.files[i] = self.files[i].rsplit(\".\",1)[0]\n",
        "            self.label_dict = label_dict\n",
        "\n",
        "        self.files = sorted(list(set(self.files)))  \n",
        "    def __getitem__(self, i):\n",
        "        # Load image from the hard disc.\n",
        "        img = PIL.Image.open(os.path.join(self.root, \n",
        "               self.files[i] + \".jpg\")).convert(\"RGB\")\n",
        "        # Load annotation file from the hard disc.\n",
        "        ann = xml_to_dict(os.path.join(self.root, \n",
        "               self.files[i] + \".xml\"))\n",
        "        # The target is given as a dict.\n",
        "        target = {}\n",
        "        target[\"boxes\"] = torch.as_tensor([[ann[\"x1\"], \n",
        "                                            ann[\"y1\"], \n",
        "                                            ann[\"x2\"], \n",
        "                                            ann[\"y2\"]]], \n",
        "                                   dtype = torch.float32)\n",
        "        target[\"labels\"]=torch.as_tensor([label_dict[ann[\"label\"]]],\n",
        "                         dtype = torch.int64)\n",
        "        target[\"image_id\"] = torch.as_tensor(i)\n",
        "        # Apply any transforms to the data if required.\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "        return img, target\n",
        "    def __len__(self):\n",
        "        return len(self.files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0cPBxWE7BLu"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms.functional as F\n",
        "import torchvision.transforms.transforms as T\n",
        "class Compose:\n",
        "    \"\"\"\n",
        "    Composes several torchvision image transforms \n",
        "    as a sequence of transformations.\n",
        "    Inputs\n",
        "        transforms: list\n",
        "            List of torchvision image transformations.\n",
        "    Returns\n",
        "        image: tensor\n",
        "        target: dict\n",
        "    \"\"\"\n",
        "    def __init__(self, transforms = []):\n",
        "        self.transforms = transforms\n",
        "    # __call__ sequentially performs the image transformations on\n",
        "    # the input image, and returns the augmented image.\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83yubrvw7F4T"
      },
      "outputs": [],
      "source": [
        "class ToTensor(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Converts a PIL image into a torch tensor.\n",
        "    Inputs\n",
        "        image: PIL Image\n",
        "        target: dict\n",
        "    Returns\n",
        "        image: tensor\n",
        "        target: dict\n",
        "    \"\"\"\n",
        "    def forward(self, image, target = None):\n",
        "        image = F.pil_to_tensor(image)\n",
        "        image = F.convert_image_dtype(image)\n",
        "        return image, target\n",
        "class RandomHorizontalFlip(T.RandomHorizontalFlip):\n",
        "    \"\"\"\n",
        "    Randomly flips an image horizontally.\n",
        "    Inputs\n",
        "        image: tensor\n",
        "        target: dict\n",
        "    Returns\n",
        "        image: tensor\n",
        "        target: dict\n",
        "    \"\"\"\n",
        "    def forward(self, image, target = None):\n",
        "        if torch.rand(1) < self.p:\n",
        "            image = F.hflip(image)\n",
        "            if target is not None:\n",
        "                width, _ = F.get_image_size(image)\n",
        "                target[\"boxes\"][:, [0, 2]] = width - \\\n",
        "                                     target[\"boxes\"][:, [2, 0]]\n",
        "        return image, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GX0grde7IGg"
      },
      "outputs": [],
      "source": [
        "def get_transform(train):\n",
        "    \"\"\"\n",
        "    Transforms a PIL Image into a torch tensor, and performs\n",
        "    random horizontal flipping of the image if training a model.\n",
        "    Inputs\n",
        "        train: bool\n",
        "            Flag indicating whether model training will occur.\n",
        "    Returns\n",
        "        compose: Compose\n",
        "            Composition of image transforms.\n",
        "    \"\"\"\n",
        "    transforms = []\n",
        "    # ToTensor is applied to all images.\n",
        "    transforms.append(ToTensor())\n",
        "    # The following transforms are applied only to the train set.\n",
        "    if train == True:\n",
        "        transforms.append(RandomHorizontalFlip(0.5))\n",
        "        # Other transforms can be added here later on.\n",
        "    return Compose(transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39reE9R87K5C"
      },
      "outputs": [],
      "source": [
        "# Train dataset. \n",
        "# Set train = True to apply the training image transforms.\n",
        "train_ds = UnderwateImagesDataset(\"/content/'underwater-images'-1/train\", get_transform(train = True))\n",
        "# Validation dataset.\n",
        "val_ds = UnderwateImagesDataset(\"/content/'underwater-images'-1/valid\", get_transform(train = False))\n",
        "# Test dataset.\n",
        "test_ds = UnderwateImagesDataset(\"/content/'underwater-images'-1/test\", get_transform(train = False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZQLmntz7-7O"
      },
      "outputs": [],
      "source": [
        "# Collate image-target pairs into a tuple.\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "# Create the DataLoaders from the Datasets. \n",
        "train_dl = torch.utils.data.DataLoader(train_ds, \n",
        "                                 batch_size = 4, \n",
        "                                 shuffle = True, \n",
        "                        collate_fn = collate_fn)\n",
        "val_dl = torch.utils.data.DataLoader(val_ds, \n",
        "                             batch_size = 4, \n",
        "                            shuffle = False, \n",
        "                    collate_fn = collate_fn)\n",
        "test_dl = torch.utils.data.DataLoader(test_ds, \n",
        "                               batch_size = 4, \n",
        "                              shuffle = False, \n",
        "                      collate_fn = collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0j2-FIeV7_7M"
      },
      "outputs": [],
      "source": [
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "def get_object_detection_model(num_classes = 4, \n",
        "                               feature_extraction = True):\n",
        "    \"\"\"\n",
        "    Inputs\n",
        "        num_classes: int\n",
        "            Number of classes to predict. Must include the \n",
        "            background which is class 0 by definition!\n",
        "        feature_extraction: bool\n",
        "            Flag indicating whether to freeze the pre-trained \n",
        "            weights. If set to True the pre-trained weights will be  \n",
        "            frozen and not be updated during.\n",
        "    Returns\n",
        "        model: FasterRCNN\n",
        "    \"\"\"\n",
        "    # Load the pretrained faster r-cnn model.\n",
        "    model = fasterrcnn_resnet50_fpn(pretrained = True)\n",
        "    # If True, the pre-trained weights will be frozen.\n",
        "    if feature_extraction == True:\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = False\n",
        "    # Replace the original 91 class top layer with a new layer\n",
        "    # tailored for num_classes.\n",
        "    in_feats = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_feats,\n",
        "                                                   num_classes)\n",
        "    for param in model.backbone.fpn.parameters():\n",
        "        param.requires_grad = True\n",
        "    for param in model.rpn.parameters():\n",
        "        param.requires_grad = True\n",
        "    for param in model.roi_heads.parameters():\n",
        "        param.requires_grad = True\n",
        "    for param in model.roi_heads.box_head.parameters():\n",
        "        param.requires_grad = True\n",
        "    for param in model.roi_heads.box_predictor.parameters():\n",
        "        param.requires_grad = True\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sb5gnhO-FSx"
      },
      "outputs": [],
      "source": [
        "def unbatch(batch, device):\n",
        "    \"\"\"\n",
        "    Unbatches a batch of data from the Dataloader.\n",
        "    Inputs\n",
        "        batch: tuple\n",
        "            Tuple containing a batch from the Dataloader.\n",
        "        device: str\n",
        "            Indicates which device (CPU/GPU) to use.\n",
        "    Returns\n",
        "        X: list\n",
        "            List of images.\n",
        "        y: list\n",
        "            List of dictionaries.\n",
        "    \"\"\"\n",
        "    X, y = batch\n",
        "    X = [x.to(device) for x in X]\n",
        "    y = [{k: v.to(device) for k, v in t.items()} for t in y]\n",
        "    return X, y\n",
        "def train_batch(batch, model, optimizer, device):\n",
        "    \"\"\"\n",
        "    Uses back propagation to train a model.\n",
        "    Inputs\n",
        "        batch: tuple\n",
        "            Tuple containing a batch from the Dataloader.\n",
        "        model: torch model\n",
        "        optimizer: torch optimizer\n",
        "        device: str\n",
        "            Indicates which device (CPU/GPU) to use.\n",
        "    Returns\n",
        "        loss: float\n",
        "            Sum of the batch losses.\n",
        "        losses: dict\n",
        "            Dictionary containing the individual losses.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    X, y = unbatch(batch, device = device)\n",
        "    optimizer.zero_grad()\n",
        "    losses = model(X, y)\n",
        "    loss = sum(loss for loss in losses.values())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss, losses\n",
        "@torch.no_grad()\n",
        "def validate_batch(batch, model, optimizer, device):\n",
        "    \"\"\"\n",
        "    Evaluates a model's loss value using validation data.\n",
        "    Inputs\n",
        "        batch: tuple\n",
        "            Tuple containing a batch from the Dataloader.\n",
        "        model: torch model\n",
        "        optimizer: torch optimizer\n",
        "        device: str\n",
        "            Indicates which device (CPU/GPU) to use.\n",
        "    Returns\n",
        "        loss: float\n",
        "            Sum of the batch losses.\n",
        "        losses: dict\n",
        "            Dictionary containing the individual losses.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    X, y = unbatch(batch, device = device)\n",
        "    optimizer.zero_grad()\n",
        "    losses = model(X, y)\n",
        "    loss = sum(loss for loss in losses.values())\n",
        "    return loss, losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nPHCEiK-Oko"
      },
      "outputs": [],
      "source": [
        "def train_fasterrcnn(model, \n",
        "                 optimizer, \n",
        "                  n_epochs, \n",
        "              train_loader, \n",
        "        test_loader = None, \n",
        "                log = None, \n",
        "               keys = None, \n",
        "            device = \"cpu\"):\n",
        "    \"\"\"\n",
        "    Trains a FasterRCNN model using train and validation \n",
        "    Dataloaders over n_epochs. \n",
        "    Returns a Report on the training and validation losses.\n",
        "    Inputs\n",
        "        model: FasterRCNN\n",
        "        optimizer: torch optimizer\n",
        "        n_epochs: int\n",
        "            Number of epochs to train.\n",
        "        train_loader: DataLoader\n",
        "        test_loader: DataLoader\n",
        "        log: Record\n",
        "            torch_snippet Record to record training progress.\n",
        "        keys: list\n",
        "            List of strs containing the FasterRCNN loss names.\n",
        "        device: str\n",
        "            Indicates which device (CPU/GPU) to use.\n",
        "    Returns\n",
        "        log: Record\n",
        "            torch_snippet Record containing the training records.\n",
        "    \"\"\"\n",
        "    if log is None:\n",
        "        log = Report(n_epochs)\n",
        "    if keys is None:\n",
        "        # FasterRCNN loss names.\n",
        "        keys = [\"loss_classifier\", \n",
        "                   \"loss_box_reg\", \n",
        "                \"loss_objectness\", \n",
        "               \"loss_rpn_box_reg\"]\n",
        "    model.to(device)\n",
        "    for epoch in range(n_epochs):\n",
        "        N = len(train_loader)\n",
        "        for ix, batch in enumerate(train_loader):\n",
        "            loss, losses = train_batch(batch, model, \n",
        "                                  optimizer, device)\n",
        "            # Record the current train loss.\n",
        "            pos = epoch + (ix + 1) / N\n",
        "            log.record(pos = pos, trn_loss = loss.item(), \n",
        "                       end = \"\\r\")\n",
        "        if test_loader is not None:\n",
        "            N = len(test_loader)\n",
        "            for ix, batch in enumerate(test_loader):\n",
        "                loss, losses = validate_batch(batch, model, \n",
        "                                         optimizer, device)\n",
        "                \n",
        "                # Record the current validation loss.\n",
        "                pos = epoch + (ix + 1) / N\n",
        "                log.record(pos = pos, val_loss = loss.item(), \n",
        "                           end = \"\\r\")\n",
        "    log.report_avgs(epoch + 1)\n",
        "    return log"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fasterrcnn(model, optimizer, n_epochs, train_loader, test_loader=None, log=None, keys=None, device=\"cpu\", patience=5):\n",
        "    \"\"\"\n",
        "    Trains a FasterRCNN model using train and validation \n",
        "    Dataloaders over n_epochs. \n",
        "    Returns a Report on the training and validation losses.\n",
        "    Inputs\n",
        "        model: FasterRCNN\n",
        "        optimizer: torch optimizer\n",
        "        n_epochs: int\n",
        "            Number of epochs to train.\n",
        "        train_loader: DataLoader\n",
        "        test_loader: DataLoader\n",
        "        log: Record\n",
        "            torch_snippet Record to record training progress.\n",
        "        keys: list\n",
        "            List of strs containing the FasterRCNN loss names.\n",
        "        device: str\n",
        "            Indicates which device (CPU/GPU) to use.\n",
        "        patience: int\n",
        "            Number of epochs with no improvement in validation loss to wait before stopping training early.\n",
        "    Returns\n",
        "        log: Record\n",
        "            torch_snippet Record containing the training records.\n",
        "    \"\"\"\n",
        "    if log is None:\n",
        "        log = Report(n_epochs)\n",
        "    if keys is None:\n",
        "        # FasterRCNN loss names.\n",
        "        keys = [\"loss_classifier\", \n",
        "                \"loss_box_reg\", \n",
        "                \"loss_objectness\", \n",
        "                \"loss_rpn_box_reg\"]\n",
        "    model.to(device)\n",
        "    \n",
        "    best_val_loss = float('inf')\n",
        "    early_stop_counter = 0\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        N = len(train_loader)\n",
        "        for ix, batch in enumerate(train_loader):\n",
        "            loss, losses = train_batch(batch, model, optimizer, device)\n",
        "            # Record the current train loss.\n",
        "            pos = epoch + (ix + 1) / N\n",
        "            log.record(pos=pos, trn_loss=loss.item(), end=\"\\r\")\n",
        "            \n",
        "        if test_loader is not None:\n",
        "            N = len(test_loader)\n",
        "            val_losses = []\n",
        "            for ix, batch in enumerate(test_loader):\n",
        "                loss, losses = validate_batch(batch, model, optimizer, device)\n",
        "                # Record the current validation loss.\n",
        "                pos = epoch + (ix + 1) / N\n",
        "                log.record(pos=pos, val_loss=loss.item(), end=\"\\r\")\n",
        "                val_losses.append(loss.item())\n",
        "            \n",
        "            avg_val_loss = sum(val_losses) / len(val_losses)\n",
        "            \n",
        "            # Check for early stopping\n",
        "            if avg_val_loss < best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "                early_stop_counter = 0\n",
        "            else:\n",
        "                early_stop_counter += 1\n",
        "                if early_stop_counter >= patience:\n",
        "                    print(f\"No improvement in validation loss for {patience} epochs. Stopping early...\")\n",
        "                    break\n",
        "                    \n",
        "    log.report_avgs(epoch + 1)\n",
        "    return log"
      ],
      "metadata": {
        "id": "fB_8a0TKET7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxskGCtu-bML"
      },
      "outputs": [],
      "source": [
        "# Create the faster rcnn model with 3 classes - dog, cat and \n",
        "# background.\n",
        "model = get_object_detection_model(num_classes = 4,   \n",
        "                        feature_extraction = False)\n",
        "# Use the stochastic gradient descent optimizer.\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, \n",
        "                        lr = 0.005, \n",
        "                    momentum = 0.9, \n",
        "             weight_decay = 0.0005)\n",
        "# Train the model over 1 epoch.\n",
        "log = train_fasterrcnn(model = model, \n",
        "               optimizer = optimizer, \n",
        "                        n_epochs = 6,\n",
        "             train_loader = train_dl, \n",
        "                test_loader = val_dl,\n",
        "             log = None, keys = None,\n",
        "                     device = device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "log.plot(x='epoch', y=['trn_loss', 'val_loss'], title='Training/Validation Loss')"
      ],
      "metadata": {
        "id": "XtBZf5cEwH87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-f_5AS8BleHD"
      },
      "outputs": [],
      "source": [
        "!pip install tensorboard\n",
        "#tensorboard --logdir=log"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensorboard --logdir=log"
      ],
      "metadata": {
        "id": "pSXS3hUAvOAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qBrsEkpXmzq"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def predict_batch(batch, model, device):\n",
        "    \"\"\"\n",
        "    Gets the predictions for a batch of data.\n",
        "    Inputs\n",
        "        batch: tuple\n",
        "            Tuple containing a batch from the Dataloader.\n",
        "        model: torch model\n",
        "        device: str\n",
        "            Indicates which device (CPU/GPU) to use.\n",
        "    Returns\n",
        "        images: list\n",
        "            List of tensors of the images.\n",
        "        predictions: list\n",
        "            List of dicts containing the predictions for the \n",
        "            bounding boxes, labels and confidence scores.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    X, _ = unbatch(batch, device = device)\n",
        "    predictions = model(X)\n",
        "    return [x.cpu() for x in X], predictions\n",
        "def predict(model, data_loader, device = \"cpu\"):\n",
        "    \"\"\"\n",
        "    Gets the predictions for a batch of data.\n",
        "    Inputs\n",
        "        model: torch model\n",
        "        data_loader: torch Dataloader\n",
        "        device: str\n",
        "            Indicates which device (CPU/GPU) to use.\n",
        "    Returns\n",
        "        images: list\n",
        "            List of tensors of the images.\n",
        "        predictions: list\n",
        "            List of dicts containing the predictions for the \n",
        "            bounding boxes, labels and confidence scores.\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    predictions = []\n",
        "    for i, batch in enumerate(data_loader):\n",
        "        X, p = predict_batch(batch, model, device)\n",
        "        images = images + X\n",
        "        predictions = predictions + p\n",
        "    \n",
        "    return images, predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k62SojxF-bU7"
      },
      "outputs": [],
      "source": [
        "def decode_prediction(prediction, \n",
        "                      score_threshold = 0.5, \n",
        "                      nms_iou_threshold = 0.5):\n",
        "    \"\"\"\n",
        "    Inputs\n",
        "        prediction: dict\n",
        "        score_threshold: float\n",
        "        nms_iou_threshold: float\n",
        "    Returns\n",
        "        prediction: tuple\n",
        "    \"\"\"\n",
        "    boxes = prediction[\"boxes\"]\n",
        "    scores = prediction[\"scores\"]\n",
        "    labels = prediction[\"labels\"]\n",
        "    # Remove any low-score predictions.\n",
        "    if score_threshold is not None:\n",
        "        want = scores > score_threshold\n",
        "        boxes = boxes[want]\n",
        "        scores = scores[want]\n",
        "        labels = labels[want]\n",
        "    # Remove any overlapping bounding boxes using NMS.\n",
        "    if nms_iou_threshold is not None:\n",
        "        want = torchvision.ops.nms(boxes = boxes, scores = scores, \n",
        "                                iou_threshold = nms_iou_threshold)\n",
        "        boxes = boxes[want]\n",
        "        scores = scores[want]\n",
        "        labels = labels[want]\n",
        "    return (boxes.cpu().numpy(), \n",
        "            labels.cpu().numpy(), \n",
        "            scores.cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "As6UAHKS-w7e"
      },
      "outputs": [],
      "source": [
        "images, predictions = predict(model, test_dl, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4Eut8lO-8ww"
      },
      "outputs": [],
      "source": [
        "print(predictions)\n",
        "img_index = 4\n",
        "boxes, labels, scores = decode_prediction(predictions[img_index])\n",
        "print(boxes)\n",
        "fig, ax = plt.subplots(figsize = [5, 5])\n",
        "ax.imshow(images[img_index].permute(1, 2, 0).numpy())\n",
        "for i, b in enumerate(boxes):\n",
        "    rect = patches.Rectangle(b[:2].astype(int),\n",
        "                             (b[2] - b[0]).astype(int),\n",
        "                             (b[3] - b[1]).astype(int),\n",
        "                             linewidth = 1,\n",
        "                             edgecolor = \"r\",\n",
        "                             facecolor = \"none\")\n",
        "    ax.add_patch(rect)\n",
        "    ax.text(b[0].astype(int),\n",
        "            b[1].astype(int) - 5,\n",
        "            \"{} : {:.3f}\".format(reverse_label_dict[labels[i]],\n",
        "            scores[i]), color = \"r\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}